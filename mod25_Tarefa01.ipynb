{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo - Tarefa 01\n",
    "\n",
    "1. Marque quais desses métodos/algoritmos muito populares em ciência de dados são baseados no uso de derivada:\n",
    "\n",
    "    1. Método Mínimos Quadrados\n",
    "    2. Gradiente descendente\n",
    "    3. Newton Raphson\n",
    "    4. CART (Árvore de decisão)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "1- Método dos Mínimos Quadrados: O Método dos Mínimos Quadrados é uma técnica estatística usada para encontrar a melhor linha (ou curva) que se ajusta aos dados minimizando a \n",
    "\n",
    "soma dos quadrados das diferenças entre os valores observados e os valores previstos pelo modelo. Embora o Método dos Mínimos Quadrados não seja diretamente baseado em derivadas \n",
    "\n",
    "para sua aplicação, ele é usado para derivar equações de minimização de erro quadrático, mas o método em si não exige o cálculo de derivadas.\n",
    "\n",
    "2- Gradiente Descendente: O Gradiente Descendente é um algoritmo de otimização usado para encontrar o mínimo de uma função. Ele usa a derivada (ou gradiente) da função objetivo\n",
    "\n",
    " para encontrar a direção na qual a função está diminuindo mais rapidamente e, em seguida, dá passos iterativos nessa direção para minimizar a função de custo. Portanto, o \n",
    "\n",
    " Gradiente Descendente é baseado diretamente no uso de derivadas para encontrar o mínimo local ou global de uma função.\n",
    "\n",
    "3- Newton-Raphson: O método de Newton-Raphson é um algoritmo de otimização utilizado para encontrar soluções aproximadas de equações não lineares. Ele utiliza a derivada da \n",
    "\n",
    "função para aproximar os zeros da função, fornecendo uma convergência rápida quando aplicável.\n",
    "\n",
    "4- CART (Árvore de Decisão): As Árvores de Decisão, como as implementadas no algoritmo CART (Classificação e Regressão por Árvores), não são diretamente baseadas no cálculo de \n",
    "\n",
    "derivadas. Elas são construídas a partir da seleção de características e divisões que melhor separam os dados com base em critérios como Gini impurity ou entropia, sem envolver \n",
    "\n",
    "cálculos de derivadas diretamente na construção da árvore.\n",
    "\n",
    "Portanto, dentre as opções fornecidas, os métodos baseados no uso de derivadas são:\n",
    "\n",
    "Gradiente descendente\n",
    "\n",
    "Newton Raphson\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Dada uma base de dados com uma variável resposta $y$ e um conjunto de variáveis explicativas. Considere uma estrutura de um modelo de regressão. Explique com suas palavras por que não é possível obter parâmetros que forneçam um erro quadrático médio (EQM) menor que o obtido com estimadores de mínimos quadrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficiente (inclinação): [1.95402268]\n",
      "Intercepto: 1.0215096157546748\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Criar dados de exemplo\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) \n",
    "y = 2 * X.squeeze() + 1 + 0.1 * np.random.randn(100) \n",
    "\n",
    "# Criar e ajustar o modelo de regressão linear\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"Coeficiente (inclinação):\", model.coef_)\n",
    "print(\"Intercepto:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.976\n",
      "Model:                            OLS   Adj. R-squared:                  0.976\n",
      "Method:                 Least Squares   F-statistic:                     4065.\n",
      "Date:                Mon, 27 Nov 2023   Prob (F-statistic):           1.35e-81\n",
      "Time:                        19:27:48   Log-Likelihood:                 99.112\n",
      "No. Observations:                 100   AIC:                            -194.2\n",
      "Df Residuals:                      98   BIC:                            -189.0\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.0215      0.017     59.988      0.000       0.988       1.055\n",
      "x1             1.9540      0.031     63.754      0.000       1.893       2.015\n",
      "==============================================================================\n",
      "Omnibus:                        0.900   Durbin-Watson:                   2.285\n",
      "Prob(Omnibus):                  0.638   Jarque-Bera (JB):                0.808\n",
      "Skew:                           0.217   Prob(JB):                        0.668\n",
      "Kurtosis:                       2.929   Cond. No.                         4.18\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "# Criar dados de exemplo\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)  # Variável independente\n",
    "y = 2 * X.squeeze() + 1 + 0.1 * np.random.randn(100)  \n",
    "\n",
    "\n",
    "X_with_intercept = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(y, X_with_intercept)\n",
    "\n",
    "results = model.fit()\n",
    "\n",
    "\n",
    "print(results.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Índice",
   "title_sidebar": "Conteúdo",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
