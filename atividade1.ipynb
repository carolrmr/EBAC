{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Módulo 24 - Tarefa 01\n",
    "\n",
    "Cite 5 diferenças entre o Random Forest e o AdaBoost\n",
    "\n",
    "Acesse o link Scikit-learn - adaboost, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo do AdaBoost\n",
    "\n",
    "Cite 5 Hyperparametros importantes no AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Árvores de Decisão vs Tocos de Decisão\n",
    "\n",
    "Random Forest faz uso de diversas árvores de decisão que possuem profundidades variadas. Já o AdaBoost utiliza tocos (Stumps) de decisão, que nada mais são do que árvores de decisão com um nó e duas folhas apenas.\n",
    "\n",
    "Amostragem de Dados\n",
    "\n",
    "Ambos os métodos utilizam amostragem de dados, porém diferem um pouco em cada caso. Random Forest utiliza o Bagging na amostra principal para seleção das amostras para as árvores \n",
    "\n",
    "de decisão. No AdaBoost cada toco de decisão tem uma amostra que depende do resultado do toco anterior devido ao sistema de pesos que existe neste modelo.\n",
    "\n",
    "Pesos\n",
    "No Random Forest as decisões tomadas por cada árvore possuem o mesmo peso na decisão final, já no AdaBoost o peso de cada toco pode variar, sendo que alguns tocos terão mais peso \n",
    "\n",
    "na decisão final do que outros.\n",
    "\n",
    "Ordem das Árvores\n",
    "\n",
    "No Random Forest as árvores de decisão são geradas de maneira independente uma das outras. No AdaBoost cada toco de decisão sofre influencia dos erros cometidos no toco de \n",
    "\n",
    "decisão anterior.\n",
    "\n",
    "Precisão da Classificação\n",
    "\n",
    "Random Forest geralmente acaba sendo mais preciso em modelos de classificação por utilizar mais de uma variável explicativa no modelo, em contraste ao AdaBoost que utiliza apenas \n",
    "\n",
    "uma variável explicativa por toco de decisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "clf = AdaBoostClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\n",
    "base_estimator\n",
    "\n",
    "Estimador base utilizado. Se None, então o estimador base é max_depth=1.\n",
    "\n",
    "n_estimators\n",
    "\n",
    "Número de estimadores no qual o boosting será realizado. Em caso de ajuste perfeito o processo de aprendizagem é interrompido precocemente.\n",
    "\n",
    "learning_rate\n",
    "\n",
    "Peso aplicado a cada classificador em cada iteração do boosting.\n",
    "\n",
    "algorithm\n",
    "\n",
    "Tipo do algoritmo boosting utilizado, escolha entre 'SAMME.R' e 'SAMME'.\n",
    "\n",
    "random_state\n",
    "\n",
    "Permite controlar a seed aleatória dada a cada base_estimator em cada iteração do boosting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
