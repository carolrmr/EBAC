{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Módulo 24 - Tarefa 02\n",
    "\n",
    "Cite 5 diferenças entre o AdaBoost e o GBM.\n",
    "\n",
    "Acesse o link Scikit-learn - GBM, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de - classificação e de regressão do GBM.\n",
    "\n",
    "Cite 5 Hyperparametros importantes no GBM.\n",
    "\n",
    "Acessando o artigo do Jerome Friedman Stochastic e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Árvores de Decisão vs Stumps de Decisão\n",
    "\n",
    "GBM faz uso de diversas árvores de decisão que possuem profundidades variadas. Já o AdaBoost utiliza Stumps de decisão, que nada mais são do que árvores de decisão com um nó e \n",
    "\n",
    "duas folhas apenas.\n",
    "\n",
    "Pesos\n",
    "\n",
    "No GBM as decisões tomadas por cada árvore possuem o mesmo peso na decisão final e são multiplicados por uma mesma constante (eta), já no AdaBoost o peso de cada Stump pode \n",
    "\n",
    "variar, sendo que alguns Stumps terão mais peso na decisão final do que outros.\n",
    "\n",
    "Primeiro Passo\n",
    "\n",
    "No AdaBoost o primeiro passo é um Stump de decisão, já no GBM primeiro tiramos a média da variável resposta pra apartir dai começarmos a criar árvores de decisão.\n",
    "\n",
    "Resíduo vs Resposta\n",
    "\n",
    "No AdaBoost cada Stump fornece uma resposta, que por média ou votação considerando os pesos resulta na resposta final, no GBM cada árvore de decisão é criada para prever o \n",
    "\n",
    "resíduo, que depois é transformado na resposta final do modelo.\n",
    "\n",
    "Base para a Árvore\n",
    "\n",
    "No AdaBoost a base de dados utilizada em cada Stump varia dependendo dos resultados obtidos no Stump anterior, no GBM cada árvore depende dos resíduos encontrados nas árvores \n",
    "\n",
    "anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, \n",
    "                                max_depth=1, random_state=0, loss='squared_error').fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\n",
    "n_estimators\n",
    "\n",
    "Número de etapas que o boosting precisa realizar. Gradient Boosting é robusto a overfitting, geralmente quanto maior o valor, melhor a performance.\n",
    "\n",
    "learning_rate\n",
    "\n",
    "Reduz a contribuição de cada árvore pelo learning_rate.\n",
    "\n",
    "min_samples_leaf\n",
    "\n",
    "O número mínimo de amostras necessárias para estar em um nó folha. Um ponto de divisão em qualquer profundidade só será considerado se deixar pelo menos min_samples_leaf amostras \n",
    "\n",
    "de treinamento em cada um dos ramos esquerdo e direito. Isso pode ter o efeito de suavizar o modelo, especialmente na regressão.\n",
    "\n",
    "max_depth\n",
    "\n",
    "A profundidade máxima dos estimadores de regressão individuais. A profundidade máxima limita o número de nós na árvore.\n",
    "\n",
    "ccp_alpha\n",
    "\n",
    "Parâmetro de complexidade usado para Minimal Cost-Complexity Pruning. A subárvore com a maior complexidade de custo que for menor do que ccp_alpha será escolhida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "A principal diferença é que no Stochastic GBM é a incorporação da aleatoriedade como parte integral do processo. Em cada iteração uma sub-amostra de base de treino é extraída de \n",
    "\n",
    "maneira aleatória (sem reposição) da base de treino completa. Etapa que não existe no GBM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
